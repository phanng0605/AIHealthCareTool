{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdK6WKzLY-8a"
      },
      "source": [
        "# Install Essential Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XU3_aRitCbl",
        "outputId": "f4ff68b1-1989-4d7a-949b-247c78e84906"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.26.5)\n",
            "Requirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (2.28.1)\n",
            "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.20->openai) (3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSfTikpK0Bu5",
        "outputId": "3d191b0c-77f4-496c-d0d8-5439080b2658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymupdf in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.21.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSYtBERigMYj"
      },
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oku1xgSZIu5"
      },
      "source": [
        "# Import API KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RfmpkMqT1daE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "OPENAI_API_KEY = ''\n",
        "with open('/Users/trongphan/Desktop/hoho/OpenAI.json', 'r') as file_to_read:\n",
        "    json_data = json.load(file_to_read)\n",
        "    OPENAI_API_KEY = json_data[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XajkO-wEtnXp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key =  OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "srz45bfF0DJ1"
      },
      "outputs": [],
      "source": [
        "import fitz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXHBDyCpgLHE"
      },
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjh6UssbZXUv"
      },
      "source": [
        "## Transformer Paper Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vfWpy7_QVC8t"
      },
      "outputs": [],
      "source": [
        "doc = fitz.open('/Users/trongphan/Downloads/Attention.pdf') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ni19lQsoVJKP"
      },
      "outputs": [],
      "source": [
        "summary_list =[]\n",
        "for page in doc:\n",
        "  text = page.get_text(\"text\")\n",
        "  #print(text)\n",
        "  prompt= text + \"\\n Tl;dr:\"\n",
        "  response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=120,\n",
        "  top_p=0.9,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=1\n",
        "  )\n",
        "  summary_list.append(response[\"choices\"][0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUjynEGNXcbT",
        "outputId": "c5ed1c80-f2ab-4955-d8c2-7830546acfbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " In this paper, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, and Łukasz Kaiser propose a new simple network architecture called the Transformer, which is based solely on attention mechanisms and eliminates recurrent and convolutional neural networks. Experiments on two machine translation tasks show that these models are superior in quality, more parallelizable, and require significantly less time to train. The model achieves 28.4 BLEU on the WMT 2014 English-to-  This paper introduces the Transformer, a model architecture that relies entirely on an attention mechanism to draw global dependencies between input and output. It allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for a short period of time.  The Transformer model architecture consists of an encoder stack with two sub-layers: a multi-head self-attention mechanism, and a position-wise fully connected feed-forward network. The decoder stack also includes a third sub-layer which performs multi-head attention over the output of the encoder stack. Attention is a function that maps a query and a set of key-value pairs to an output vector, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. \n",
            "Scaled Dot-Product Attention is an attention mechanism that computes the dot product of queries and keys, divides each by the square root of the dimensionality of the key (√dk), and applies a softmax function to obtain the weights on the values. Multi-Head Attention consists of multiple attention layers running in parallel, projecting the queries, keys, and values to different dimensions and performing the attention function on each one in order to generate the final output values.  The Transformer model uses multi-head attention in three different ways: encoder-decoder attention layers, self-attention layers in the encoder, and self-attention layers in the decoder. In addition to this, it also employs position-wise feed-forward networks, embeddings, and positional encoding. \n",
            "\n",
            "This paper discusses the use of self-attention layers as a viable alternative to recurrent and convolutional layers for sequence transduction tasks. Self-attention layers have the advantage of being computationally efficient, requiring only O(1) sequential operations, and having shorter maximum path lengths between long-range dependencies in the network. Furthermore, they can be used with sequences of any length, making them well suited for tasks involving very long sequences.  We describe a neural machine translation model which uses a self-attention mechanism to connect the encoder and decoder. We train our models on the standard WMT 2014 English-German and English-French datasets, using Adam optimizer with residual dropout and varying learning rate over the course of training. Our results show that the attention model outperforms conventional sequence-to-sequence models.  The Transformer model achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. The big transformer model outperforms all previously reported models by more than 2.0 BLEU, achieving a new state-of-the-art BLEU score of 28.4 for English-to-German and 41.0 for English-to-French. We also evaluated the importance of different components of the Transformer, finding that single-head attention  We observe that reducing the attention key size hurts model quality, indicating that a more sophisticated compatibility function than dot product may be beneficial. We also observe that larger models are better and dropout is helpful in avoiding overfitting. Finally, we find that replacing our sinusoidal positional encoding with learned positional embeddings results in nearly identical results to the base model.  In this paper, we presented the Transformer, the first sequence transduction model based entirely on attention. The model outperforms previously reported models on the WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks. We also showed that the model can be trained quickly and performs surprisingly well when trained on a small dataset such as the Wall Street Journal portion of the Penn Treebank. \n",
            "Recurrent neural networks are commonly used for sequence modeling tasks such as machine translation, language modeling, and speech recognition. They are able to capture long-term dependencies in data by using gating mechanisms and memory components. Recent advances in recurrent neural networks include the use of convolutional layers, self-attention, and factorization tricks to improve performance. Additionally, applications of recurrent neural networks have been explored in grammar induction and parsing. \n",
            " \n",
            " Recent advances in natural language processing have enabled the development of powerful neural networks for text processing tasks such as machine translation, summarization, and parsing. These models leverage techniques such as embeddings, attention mechanisms, tree-structured models, and reinforcement learning to produce accurate results. Additionally, improvements in deep learning architectures such as recurrent neural networks, convolutional neural networks, and long short-term memory networks have been instrumental in further advancing NLP capabilities.  Attention visualizations in layer 5 of the encoder self-attention show that many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’.  Two attention heads in layer 5 are involved in anaphora resolution, with the attentions being sharp for the word \"its.\"  The encoder self-attention layers in a Transformer model seem to have learned to perform different tasks related to the structure of the sentence, as evidenced by two examples from two different heads at layer 5 of 6.\n"
          ]
        }
      ],
      "source": [
        "summary_text=' '.join(summary_list)\n",
        "print(summary_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrN3AgYzZHzd",
        "outputId": "cf0c6b8e-3e1e-4403-9f7c-7c2a34f1459e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " This paper introduces the Transformer, a novel neural network architecture based on attention mechanisms that eliminates recurrent and convolutional neural networks. Experiments on two machine translation tasks show that these models are superior in quality, more parallelizable, and require significantly less time to train. The model achieves 28.4 BLEU on the WMT 2014 English-to-German dataset and 41.0 BLEU on the WMT 2014 English-to-French dataset. It also shows that self-attention layers can be used for sequence transduction tasks and can learn different tasks related to the structure of the sentence.\n"
          ]
        }
      ],
      "source": [
        "prompt= summary_text + \"\\n Tl;dr:\"\n",
        "response = openai.Completion.create(\n",
        "model=\"text-davinci-003\",\n",
        "prompt=prompt,\n",
        "temperature=0.7,\n",
        "max_tokens=400,\n",
        "top_p=0.9,\n",
        "frequency_penalty=0.0,\n",
        "presence_penalty=1\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np0zcc0IajDP",
        "outputId": "a616f7b7-4cd5-4d2a-a84e-66c9286b924f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" This paper introduces the Transformer, a novel neural network architecture based on attention mechanisms that eliminates recurrent and convolutional neural networks. Experiments on two machine translation tasks show that these models are superior in quality, more parallelizable, and require significantly less time to train. The model achieves 28.4 BLEU on the WMT 2014 English-to-German dataset and 41.0 BLEU on the WMT 2014 English-to-French dataset. It also shows that self-attention layers can be used for sequence transduction tasks and can learn different tasks related to the structure of the sentence.\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1677347266,\n",
            "  \"id\": \"cmpl-6nsw6uGfWqR2vxkV0D8wxJkZpsol6\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 123,\n",
            "    \"prompt_tokens\": 1209,\n",
            "    \"total_tokens\": 1332\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r4WmT6W5ePIk"
      },
      "outputs": [],
      "source": [
        "summary_list =[]\n",
        "for page in doc:\n",
        "  text = page.get_text(\"text\")\n",
        "  #print(text)\n",
        "  prompt= \"Summarize this for a second-grade student: \" +text \n",
        "  response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=120,\n",
        "  top_p=0.9,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=1\n",
        "  )\n",
        "  summary_list.append(response[\"choices\"][0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUYSaUW6f8-H",
        "outputId": "6aa8416a-370e-440f-e116-50230b9a3816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "This article talks about a new type of network, called the Transformer, which uses attention mechanisms instead of recurrence or convolutions to help people translate words from one language to another. Experiments with this new model showed that it produced better results than the existing models and required less time to train. It also works well for other tasks like English constituency parsing. \n",
            "Transduction problems such as language modeling and machine translation involve computers understanding and translating words or symbols. Scientists have been working on ways to make this easier, like using recurrent models to factor computation along the symbol positions of the input and output sequences. Attention mechanisms help understand connections between different positions in the input and output. Recently, a model called the Transformer was created which uses self-attention to draw global dependencies between input and output, allowing for more parallelization and better translation quality. A Transformer is a type of model architecture that uses attention and stacks of layers to process input. The encoder stack consists of multiple layers with two sub-layers each, while the decoder stack includes an extra sub-layer for multi-head attention over the output of the encoder. Attention functions map queries and key-value pairs to outputs and calculate the weight assigned to each value by comparing it to the query. \n",
            "Scaled Dot-Product Attention and Multi-Head Attention are two ways of helping computers understand how things are related. Scaled Dot-Product Attention takes queries, keys and values and divides the dot products by a special number to make sure the computer understands the relationships between them. Multi-Head Attention takes the queries, keys and values and splits them up into different parts to help the computer better understand the relationships. tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms\n",
            "of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the\n",
            "embeddings, so that the two can be summed. There are many choices of positional encodings, learned\n",
            "and fixed [2].\n",
            "\n",
            "In multi-head attention, the model looks at different parts of a sentence or input information from different angles. This allows it to understand the sentence better than with just This is a comparison between different types of layers used to transform one sequence of symbols into another. Self-attention layers can connect all positions with the same number of operations, which is faster than recurrent layers. Convolutional layers are also faster than recurrent layers but can only consider nearby positions. \n",
            "We are trying to make a model that can better connect input and output positions in a network. To do this, we use a combination of self-attention layers and point-wise feed-forward layers. We use these layers to train our model on large datasets and use special techniques like dropout to help improve the accuracy. A second-grade student can understand that the Transformer model is a new way of translating from one language to another. It does this better than other models and it takes less time and energy to train the model. constraints, and the training data is limited.\n",
            "\n",
            "The Transformer architecture can be used to generalize to other tasks such as English constituency parsing. This task is challenging because the output has strong structural constraints and the training data is limited. Experiments show that the Transformer does well with this task, achieving similar results to other models. A new type of model called the Transformer was developed to help with translation tasks. It is based on attention and can be trained faster than other models. It outperformed all previously reported models in English-to-German and English-to-French translation tasks. Four scientists, Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio, did a study on how to make computers understand language better. They used something called gated recurrent neural networks. Other scientists also studied how to make computers understand language better using different techniques. This is about a group of people who did research in natural language processing. They studied things like abstractive summarization, tree annotation, language models, neural machine translation, and parsing. \n",
            "Many American governments have passed new laws since 2009 to make it more difficult for people to register or vote. \n",
            "The Law should always be applied in a fair and just way, even though it can never be perfect. This is something we are missing, in my opinion. The law can never be perfect, but it should be applied justly. This is what we are missing in our world today.\n"
          ]
        }
      ],
      "source": [
        "summary_text= ' '.join(summary_list)\n",
        "print(summary_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "s = input(\"Type something you want to ask: \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nScXhI01gDiK",
        "outputId": "a8972e26-0030-46b1-da36-7628281df8a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " We need to make sure the law is applied in a way that benefits everyone, not just those in power. We need to ensure that people's voices are heard and that the law is used for good instead of evil.\n"
          ]
        }
      ],
      "source": [
        "prompt= s+summary_text \n",
        "response = openai.Completion.create(\n",
        "model=\"text-davinci-003\",\n",
        "prompt=prompt,\n",
        "temperature=0.7,\n",
        "max_tokens=400,\n",
        "top_p=0.9,\n",
        "frequency_penalty=0.0,\n",
        "presence_penalty=1\n",
        ")\n",
        "print(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_M1V45vgA7N"
      },
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO61cejsZioP"
      },
      "source": [
        "## CardioVascular Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# -----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6c6rKFDDY40V"
      },
      "outputs": [],
      "source": [
        "doc_cardio = fitz.open('/Users/trongphan/Downloads/CardioVascular.pdf') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1Ghu-iwXY6HV"
      },
      "outputs": [],
      "source": [
        "summary_list_cardio =[]\n",
        "for page in doc_cardio:\n",
        "  text = page.get_text(\"text\")\n",
        "  #print(text)\n",
        "  prompt= text + \"\\n Tl;dr:\"\n",
        "  response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=120,\n",
        "  top_p=0.9,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=1\n",
        "  )\n",
        "  summary_list_cardio.append(response[\"choices\"][0][\"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "L0sOJwlHaLzE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " This article reviews the role of genetics in cardiovascular disease, including stroke. It looks at the evidence for a heritable component to the disease, as well as how knowledge of single gene disorders can help us understand more common cardiovascular diseases. It provides an overview of the genetic basis of specific diseases and discusses the implications for patient care.  Monogenic diseases such as familial hypercholesterolemia and sitosterolemia, which are caused by mutations in genes that regulate the activity of hepatic LDL receptors and ATP-binding cassette transporters, can lead to elevated levels of LDL cholesterol and subsequent coronary artery disease.  Hypercholesterolemia is a disorder caused by defects in a putative hepatic adaptor protein, resulting in elevated plasma LDL levels. The majority of cases in the general public are attributable to high-fat diets and susceptibility/modifier genes. Hypertension is the most common disease in industrialized nations, with a prevalence above 20 percent. It increases the risk of stroke, heart attack, and renal failure, and can be reduced with treatment. Monogenic diseases that elevate or lower blood pressure can be due to molecular mechanisms in the kidney.  Investigation of rare mendelian forms of blood pressure variation has been very informative and has identified mutations in genes that regulate pathways related to hypertension or hypotension. These include mutations in the WNK family of serine–threonine kinases, aldosterone synthase, and various ion channels and transporters, which have enabled the identification of new targets for antihypertensive therapy.  Hypertrophic cardiomyopathy is a monogenic cardiac disorder that can be caused by mutations in sarcomeric proteins. Factor V Leiden increases the risk of myocardial infarction, stroke, and venous thrombosis in men and has an allele frequency of 2 to 7 percent in European populations. The pathologic features of hypertrophic cardiomyopathy consist of marked left ventricular hypertrophy, thickened ventricular septum, atrial enlargement, and a small left ventricular cavity. Investigators have found multiple causative mutations in at least 10 different sar  Mutations in the SCN5A gene, which encodes alpha subunits that form sodium channels responsible for initiating cardiac action potentials, can cause familial forms of arrhythmias such as long-QT syndrome, idiopathic ventricular fibrillation, and cardiac-conduction disease. A variant of this gene, with a transversion of cytosine to adenine in codon 1102, has been associated with arrhythmia in black Americans. This variant allele (Y1102) accelerates sodium-channel activation and increases the likelihood of abnormal cardiac rep \n",
            "Recent studies point to the usefulness of molecular markers for predicting susceptibility to arrhythmia in people with acquired or genetic risk factors. These markers can be used to identify inherited arrhythmia-susceptibility genes that encode cardiac ion channels, and polymorphisms associated with inherited forms of long-QT syndrome have been found to increase the risk of acquired arrhythmias. Current research on complex cardiovascular traits focuses on identifying genetic variants that enhance susceptibility to certain conditions. Polymorphism-association studies are being used to investigate genetic variants in candidate genes for various cardiovascular diseases, but caution \n",
            "Three studies have shown that single-nucleotide polymorphisms in genes encoding thrombospondin, adrenergic-receptor, and connexin 37 can increase the risk of myocardial infarction. Additionally, two polymorphisms in the adrenergic-receptor gene can lead to an increased risk of heart failure in black Americans. These findings suggest that genotyping can be used to identify persons at risk and guide treatment.  Genome-wide approaches are used to determine the role of genetic variants in individual responses to drugs, and have provided data showing that genetic polymorphisms of proteins involved in drug metabolism, transporters, and targets have important effects on the efficacy of cardiovascular drugs. Gene expression profiling is also a useful tool to establish molecular diagnoses, dissect the pathophysiologic features of a disease, and predict patient response to therapy. Additionally, gene expression studies have been used to define a role for proliferative and inflammatory genes in the development of restenosis after coronary artery stenting. \n",
            " Genetic diagnosis is not widely available for the diagnosis of monogenic cardiovascular disorders. Current initiatives are focusing on identifying high risk individuals, asymptomatic carriers, and unaffected family members. Research is also being done to identify DNA sequence variations in common cardiovascular diseases. Funding and disclosures are provided by the National Heart, Lung, and Blood Institute. \n",
            "This article discusses the various causes of cardiovascular disease, such as familial hypercholesterolemia, dietary cholesterol accumulation, mutations in ABC transporters, and mutations in WNK kinases. It also reviews research on treatments for hypertension, including angiotensin-converting enzyme inhibitors, calcium channel blockers, and diuretics. Finally, it examines the role of a chimaeric 11 beta-hydroxylase/aldosterone synthase gene in glucocorticoid-remediable aldosteronism and human hypertension. \n",
            " Cardiovascular Disease research has identified a number of mutations in genes that lead to various forms of the condition, such as hereditary hypertension caused by chimaeric gene duplications and ectopic expression of aldosterone synthase, defects in the P-450C18 gene, mutations in the beta subunit of the epithelial sodium channel, mutations in the thiazide-sensitive Na-Cl cotransporter, and resistance to activated protein C. These discoveries have helped to improve understanding and treatment of cardiovascular disease. \n",
            " Cardiovascular disease is a condition that can be caused by several factors, including genetic mutations in the gene for factor V, activated protein C resistance, prothrombin gene mutation, and hypertrophic cardiomyopathy. Research has shown that these mutations increase the risk of thrombosis, myocardial infarction, stroke, and venous thrombosis. Treatment options include medications and lifestyle changes to reduce the risk of developing cardiovascular diseases. \n",
            " \n",
            "Research has identified mutations in sarcomere protein genes, beta cardiac myosin heavy chain gene missense mutation, and cardiac troponin I gene as causes of dilated cardiomyopathy and hypertrophic cardiomyopathy. Additionally, research has shown a correlation between angiotensin-I converting enzyme genotypes and left ventricular hypertrophy in patients with hypertrophic cardiomyopathy. \n",
            "\n",
            "Mutations in genes associated with the renin-angiotensin-aldosterone system, cardiac troponin T, and SCN5A have been found to be associated with hypertrophic cardiomyopathy and various cardiac arrhythmias. These mutations can lead to left ventricular hypertrophy, long QT syndrome, idiopathic ventricular fibrillation, and other conduction defects. \n",
            "This article discusses the various mutations and studies that have been conducted to understand the causes of cardiovascular disease. It looks at specific genes, such as SCN5A, HERG, KVLQT1, KCNE1, and KCNE2, and how they interact with each other to create the I potassium channel, which is linked to cardiac arrhythmia. The article also covers high-throughput screening techniques, such as mass spectrometry genotyping and BeadArray technology, which can help identify potential gene variants associated with the disease.  Cardiovascular disease is a major public health concern and has been linked to certain genetic polymorphisms. Research has suggested that single nucleotide polymorphisms in coding regions of human genes, as well as polymorphisms in thrombospondin genes, β- and α-adrenergic receptors, and the angiotensin-converting enzyme gene may be associated with an increased risk of cardiovascular disease. Furthermore, studies have suggested that genotype may influence response to chronic antihypertensive treatment and drug disposition, targets, and side effects.  The insertion/deletion polymorphism of the angiotensin-converting enzyme gene can affect the progression of cardiovascular disease, and response to treatment with ACE inhibitors. Studies have shown that it affects heart failure, left ventricular hypertrophy, endothelial dysfunction, IgA nephropathy, chronic proteinuric nephropathies, and coronary atherosclerosis.  Several studies have shown that common polymorphisms of the β-adrenergic receptor can affect vascular desensitization, receptor expression, and in vivo responsiveness. Additionally, Apolipoprotein E genotypes and Cholesteryl ester transfer protein gene variants have been linked to progression of coronary atherosclerosis, plasma lipid response to atorvastatin, and antibiotic-induced cardiac arrhythmia. Beta(2)-adrenergic receptor pharmacogenetics have also been studied.  This article discusses cardiovascular disease, specifically hypertension (nephrology and cardiology), anticoagulation/ thromboembolism (pulmonary/ critical care and cardiology), lipids, and gene expression profiling. It also provides references to studies regarding the impact of laboratory molecular diagnosis on contemporary diagnostic criteria for genetically transmitted cardiovascular diseases, such as hypertrophic cardiomyopathy, long-QT syndrome, and Marfan syndrome.  This article from the New England Journal of Medicine discusses different aspects of cardiovascular disease, including arrhythmias, pacemakers and defibrillators, coronary disease and myocardial infarction, cardiology, cardiomyopathy, myocarditis and pericarditis, and genetics. It was published during the week of July 3, 2003.  Cardiovascular Disease (CVD) is a major health problem and is the leading cause of death in many countries. The risk of developing CVD can be reduced by lifestyle modifications, such as healthy eating, physical activity, and avoiding smoking. However, it is also important to be aware of the latest research and treatments for CVD, which can help reduce the risk of developing and dying from the disease. In this article, we review the latest research on CVD, including its causes, diagnosis, prevention, and treatment.\n"
          ]
        }
      ],
      "source": [
        "summary_text_cardio=' '.join(summary_list_cardio)\n",
        "print(summary_text_cardio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbxHoTB0afoY"
      },
      "outputs": [],
      "source": [
        "summary_list =[]\n",
        "for page in doc:\n",
        "  text = page.get_text(\"text\")\n",
        "  #print(text)\n",
        "  prompt= \"Give the name of the author\" +text \n",
        "  response = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  prompt=prompt,\n",
        "  temperature=0.7,\n",
        "  max_tokens=120,\n",
        "  top_p=0.9,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=1\n",
        "  )\n",
        "  summary_list.append(response[\"choices\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
